import streamlit as st
import os

from langchain_community.llms import Ollama
from document_loader import load_documents_into_database

from models import get_list_of_models

from llm import getStreamingChain


EMBEDDING_MODEL = "nomic-embed-text"
PATH = "E:/pdf"


st.title("Local LLM with RAG üìö")

if "list_of_models" not in st.session_state:
    st.session_state["list_of_models"] = get_list_of_models()

selected_model = st.sidebar.selectbox(
    "Ch·ªçn 1 model:", st.session_state["list_of_models"]
)

if st.session_state.get("ollama_model") != selected_model:
    st.session_state["ollama_model"] = selected_model
    st.session_state["llm"] = Ollama(model=selected_model)


# Folder selection
folder_path = st.sidebar.text_input("Nh·∫≠p ƒë∆∞·ªùng d·∫´n th∆∞ m·ª•c:", PATH)

if folder_path:
    if not os.path.isdir(folder_path):
        st.error(
            "ƒê∆∞·ªùng d·∫´n ƒë√£ cung c·∫•p kh√¥ng ph·∫£i l√† th∆∞ m·ª•c h·ª£p l·ªá. Vui l√≤ng nh·∫≠p m·ªôt ƒë∆∞·ªùng d·∫´n th∆∞ m·ª•c h·ª£p l·ªá."
        )
    else:
        if st.sidebar.button("Ch·ªâ m·ª•c t√†i li·ªáu"):
            if "db" not in st.session_state:
                with st.spinner(
                    "T·∫°o embedding v√† t·∫£i t√†i li·ªáu v√†o Chroma"
                ):
                    st.session_state["db"] = load_documents_into_database(
                        EMBEDDING_MODEL, folder_path
                    )
                st.info("S·∫µn s√†ng ƒë·ªÉ tr·∫£ l·ªùi c√°c c√¢u h·ªèi!")
else:
    st.warning("Vui l√≤ng nh·∫≠p ƒë∆∞·ªùng d·∫´n th∆∞ m·ª•c ƒë·ªÉ t·∫£i t√†i li·ªáu v√†o c∆° s·ªü d·ªØ li·ªáu")

# Initialize chat history
if "messages" not in st.session_state:
    st.session_state.messages = []

# Display chat messages from history on app rerun
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("Question"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        stream = getStreamingChain(
            prompt,
            st.session_state.messages,
            st.session_state["llm"],
            st.session_state["db"],
        )
        response = st.write_stream(stream)
        st.session_state.messages.append({"role": "assistant", "content": response})
